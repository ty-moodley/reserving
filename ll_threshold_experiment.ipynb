{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88329157",
   "metadata": {},
   "source": [
    "# Large Loss Threshold Implementation\n",
    "The Jupyter Notebook aims to implement a reserving methodology which focuses on valuing attritional claims and large claims separately. In theory, the removal of large losses will result in a more stable attritional triangle which should yield better predictions of future claims development. \n",
    "\n",
    "Large losses will be defined as claims above a threshold determined by the largest change in Mack Standard Error relative to the removal of the largest claims in the data. Claim size will be defined as the following:\n",
    "- For incurred triangles, claim size of an individual claim is calculated as the cumulative value of paid claims and the maximum OCR amount held at any point\n",
    "- For paid triangles, claim size of an individual claim is calculated as the cumulative value of paid claims\n",
    "\n",
    "Large claims above the threshold should be assessed separately and a separate \"large\" IBNR should be raised to support these claims.\n",
    "\n",
    "The triangle is incurred and thus will focus on an incurred approach. Where this approach is applied to paid data instead, the use of OCR in the threshold process should be removed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b401a5",
   "metadata": {},
   "source": [
    "## 0. Setting up the Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c38aec",
   "metadata": {},
   "source": [
    "### 0.1. Use a virtual environment (Optional)\n",
    "A virtual environment is a useful method of ensuring that any packages installed for code are specfic to the current project. We will use `venv` for this project and a list of packages can then be installed using the `requirements.txt` file. This will reduce the need of individually installing each package and will ensure that the environment is appropriate for running the code. Use the following steps:\n",
    "\n",
    "1. Install the virtualenv package using the command below\n",
    "2. Open the terminal and run the command `virtualenv venv` which will create a folder called venv with the required information\n",
    "3. Run the command `.\\venv\\Scripts\\activate` in the terminal which will active the virtual environment\n",
    "4. Packages can then be installed through pip individually (like the virtualenv package below) or through the `requirement.txt` file (demonstrated 2 cells below)\n",
    "5. The virtualenv package can stopped at any time by running the command \"deactivate\" in the terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1be0763",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install virtualenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab95f23e",
   "metadata": {},
   "source": [
    "If using the virtual environment, follow steps 2 and 3 above before proceeding. Instructions are based on powershell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a116be08",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r .\\requirement.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465a139c",
   "metadata": {},
   "source": [
    "### 0.2. Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d54e939",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import chainladder as cl\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from datetime import date, timedelta\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import sqlite3\n",
    "\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "warnings.filterwarnings('ignore', category=RuntimeWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e1ea937",
   "metadata": {},
   "source": [
    "### 0.3. Important variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32327baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"\"\n",
    "\n",
    "conn = sqlite3.connect(':memory:')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5007c9",
   "metadata": {},
   "source": [
    "## 1. Determining Large Loss Threshold\n",
    "\n",
    "Outline of methodology:\n",
    "- Organise claims data\n",
    "- Remove x% of largest claims\n",
    "- Calculate mack error\n",
    "- Iteratively remove x% of claims and calculate mack error\n",
    "- Plot the mack error and % of claims removed (rebase from 0 to 1 for both)\n",
    "- Select the threshold which results in the steepest gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c3e456",
   "metadata": {},
   "source": [
    "### 1.1. Extracting and organising data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9a47aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the Gross Earned Premium Data\n",
    "gep_data = pd.read_excel(data_path, sheet_name=\"RW Gross EP\").drop(columns=[\"Unnamed: 0\", \"Unnamed: 12\"])\n",
    "gep_data = pd.melt(gep_data, id_vars=[\"Accident Period\"], var_name=\"Segment\", value_name=\"GEP\")\n",
    "gep_data['Segment'] = gep_data['Segment'].str.title()\n",
    "gep_col = gep_data.columns\n",
    "gep_data = gep_data.rename(columns={\n",
    "    gep_col[0]:\"loss_date\",\n",
    "    gep_col[1]:\"segment\",\n",
    "    gep_col[2]:\"gep\"\n",
    "})\n",
    "\n",
    "gep_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8470f2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the Net Earned Premium Data\n",
    "nep_data = pd.read_excel(data_path, sheet_name=\"RW Net EP\").drop(columns=[\"Unnamed: 0\", \"Unnamed: 12\"])\n",
    "nep_data = pd.melt(nep_data, id_vars=[\"Accident Period\"], var_name=\"Segment\", value_name=\"NEP\")\n",
    "nep_data['Segment'] = nep_data['Segment'].str.title()\n",
    "nep_col = nep_data.columns\n",
    "nep_data = nep_data.rename(columns={\n",
    "    nep_col[0]:\"loss_date\",\n",
    "    nep_col[1]:\"segment\",\n",
    "    nep_col[2]:\"nep\"\n",
    "})\n",
    "\n",
    "nep_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874e0361",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining the Gross and Net Earned Premium Data\n",
    "combined_ep = gep_data.merge(nep_data, on=[\"loss_date\", \"segment\"])\n",
    "assert gep_data.shape == nep_data.shape\n",
    "assert gep_data.shape[0] == combined_ep.shape[0]\n",
    "\n",
    "combined_ep.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7928f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the Gross and Net Paid Claims Data\n",
    "paid_data = pd.read_excel(data_path, sheet_name=\"RW Gross_Net Paid 122023\")\n",
    "paid_col = paid_data.columns\n",
    "paid_data = paid_data.rename(columns={\n",
    "    paid_col[0]:\"claim_id\",\n",
    "    paid_col[1]:\"segment\",\n",
    "    paid_col[2]:\"loss_date\", \n",
    "    paid_col[3]:\"transaction_date\",\n",
    "    paid_col[4]:\"gross_paid\",\n",
    "    paid_col[5]:\"net_paid\"\n",
    "})\n",
    "paid_data['segment'] = paid_data['segment'].str.title()\n",
    "\n",
    "paid_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38de0826",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the Gross and Net Outstanding Claims Data\n",
    "\n",
    "ocr_data = pd.read_excel(data_path, sheet_name=\"RW Gross_Net RBNS 122023\")\n",
    "\n",
    "ocr_col = ocr_data.columns\n",
    "ocr_data = ocr_data.rename(columns={\n",
    "    ocr_col[0]:\"claim_id\",\n",
    "    ocr_col[1]:\"segment\",\n",
    "    ocr_col[2]:\"loss_date\", \n",
    "    ocr_col[3]:\"transaction_date\",\n",
    "    ocr_col[4]:\"gross_ocr\",\n",
    "    ocr_col[5]:\"net_ocr\"\n",
    "})\n",
    "ocr_data[\"loss_date\"] = pd.to_datetime(ocr_data[\"loss_date\"])\n",
    "ocr_data['segment'] = ocr_data['segment'].str.title()\n",
    "\n",
    "ocr_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96b5a21",
   "metadata": {},
   "source": [
    "Noted when looking at the data that there were 5 claims in which the loss date changed for one of the transactions, however, this occured once towards the later periods. This issue will be corrected by using the loss date associated with the earliest transaction date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef3d92d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ocr_data_sorted = ocr_data.sort_values(by=['claim_id', 'transaction_date'])\n",
    "\n",
    "# Drop duplicates to keep the first occurrence for each 'claim_id'\n",
    "ocr_data_sorted = ocr_data_sorted.drop_duplicates(subset=['claim_id'], keep='first')[[\"claim_id\", \"loss_date\"]]\n",
    "ocr_data_sorted.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0be82d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "ocr_data = ocr_data.drop(columns=[\"loss_date\"]).merge(ocr_data_sorted, how=\"left\", on=[\"claim_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7866a2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "ocr_data.to_sql('ocr_claims', conn, index=False, if_exists='replace')\n",
    "paid_data.to_sql('paid_claims', conn, index=False, if_exists='replace')\n",
    "combined_ep.to_sql('earned_premiums', conn, index=False, if_exists='replace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6a240e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    pd.read_sql_query(\n",
    "    \"\"\"\n",
    "        SELECT \n",
    "        segment, \n",
    "        SUM(CASE WHEN loss_date IS NULL THEN gross_ocr ELSE 0 END) AS ocr_amount_blank_loss_date,\n",
    "        SUM(CASE WHEN loss_date IS NOT NULL THEN gross_ocr ELSE 0 END) AS ocr_amount_non_blank_loss_date,\n",
    "        SUM(CASE WHEN loss_date IS NULL THEN gross_ocr ELSE 0 END) * 1.0 / SUM(gross_ocr) AS proportion_blank_to_all\n",
    "        FROM ocr_claims\n",
    "        GROUP BY segment;\n",
    "    \"\"\", conn)\n",
    ")\n",
    "\n",
    "# Negligible proportion of blank loss date claims, blank loss data claims only noted in Engineering segment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "338a7188",
   "metadata": {},
   "source": [
    "### 1.1. Organising Claims Data\n",
    "\n",
    "The Rwanda valuation data provided consists of paid and OCR data. In assessing the data, the following has been noted:\n",
    "- Paid data is incremental\n",
    "- OCR data is cumulative\n",
    "\n",
    "This was assessed by recreating the triangles for the December valuations with no differences noted. Instances where a loss date was blank resulted in the data being removed from the triangle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb41af61",
   "metadata": {},
   "outputs": [],
   "source": [
    "ocr_claims = pd.read_sql_query(\n",
    "    \"\"\"\n",
    "        SELECT \n",
    "        claim_id,\n",
    "        segment,\n",
    "        loss_date,\n",
    "        MAX(gross_ocr) as gross_amount,\n",
    "        MAX(net_ocr) as net_amount\n",
    "        FROM ocr_claims\n",
    "        GROUP BY claim_id, segment, loss_date;\n",
    "    \"\"\", conn).dropna(subset=[\"loss_date\"])\n",
    "\n",
    "ocr_claims.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4cd04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "paid_claims = pd.read_sql_query(\n",
    "    \"\"\"\n",
    "        SELECT \n",
    "        claim_id,\n",
    "        segment,\n",
    "        loss_date,\n",
    "        SUM(gross_paid) as gross_amount,\n",
    "        SUM(net_paid) as net_amount\n",
    "        FROM paid_claims\n",
    "        GROUP BY claim_id, segment, loss_date;\n",
    "    \"\"\", conn).dropna(subset=[\"loss_date\"])\n",
    "\n",
    "paid_claims.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177bd614",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = (\n",
    "    pd.concat([ocr_claims, paid_claims])\n",
    ")\n",
    "\n",
    "combined_df = combined_df.groupby(['claim_id','segment', 'loss_date']).agg(\n",
    "    gross_amount=pd.NamedAgg(column=\"gross_amount\", aggfunc=\"sum\"),\n",
    "    net_amount=pd.NamedAgg(column=\"net_amount\", aggfunc=\"sum\")\n",
    "    ).reset_index()\n",
    "combined_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66605d8d",
   "metadata": {},
   "source": [
    "### 1.2. Identifying Largest Claims "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289f8f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "segment_list = ocr_claims[\"segment\"].unique()\n",
    "plot_path = \"./plots\"\n",
    "threshold_percentage = {}\n",
    "\n",
    "if not os.path.exists(plot_path):\n",
    "    os.makedirs(plot_path)\n",
    "\n",
    "for segment in segment_list[:4]:\n",
    "    claims_percentage = []\n",
    "    mack_mse = []\n",
    "    threshold = []\n",
    "\n",
    "    start_point = 1\n",
    "    end_point = 0.7\n",
    "    increment = 0.01\n",
    "    interval = int(round((start_point-end_point)/increment,0))\n",
    "\n",
    "    segment_df = combined_df.loc[combined_df[\"segment\"]==segment]\n",
    "    segment_paid = paid_data.loc[paid_data[\"segment\"]==segment]\n",
    "    segment_ocr = ocr_data.loc[ocr_data[\"segment\"]==segment]\n",
    "    \n",
    "    print(f\"{segment}: {interval}\")\n",
    "\n",
    "    for i in range(interval+1):\n",
    "        attritional_claims = segment_df.sort_values(by='gross_amount', ascending=False)\n",
    "        one_percent_index = int(len(attritional_claims) * increment*i)\n",
    "        attritional_claims = attritional_claims.iloc[one_percent_index:]\n",
    "\n",
    "        if one_percent_index>0:\n",
    "            segment_paid = segment_paid.merge(attritional_claims[\"claim_id\"], on=\"claim_id\")\n",
    "            segment_ocr = segment_ocr.merge(attritional_claims[\"claim_id\"], on=\"claim_id\")\n",
    "\n",
    "        paid_triangle = cl.Triangle(\n",
    "            data=segment_paid,\n",
    "            origin=\"loss_date\",\n",
    "            development=\"transaction_date\",\n",
    "            columns=[\"gross_paid\"],\n",
    "            cumulative=False\n",
    "        ).grain(\"OQDQ\").incr_to_cum()\n",
    "\n",
    "        ocr_triangle = cl.Triangle(\n",
    "            data=segment_ocr,\n",
    "            origin=\"loss_date\",\n",
    "            development=\"transaction_date\",\n",
    "            columns=[\"gross_ocr\"],\n",
    "            cumulative=True\n",
    "        ).grain(\"OQDQ\")\n",
    "\n",
    "        combined_triangle = paid_triangle + ocr_triangle\n",
    "        \n",
    "        mack = cl.MackChainladder()\n",
    "        dev = cl.Development(average='volume')\n",
    "        mack.fit(dev.fit_transform(combined_triangle))\n",
    "        plot_data = mack.summary_.to_frame(origin_as_datetime=False)\n",
    "        # print(plot_data[\"Mack Std Err\"].sum())\n",
    "        \n",
    "        claims_percentage.append(start_point-increment*i)\n",
    "        mack_mse.append(plot_data[[\"Mack Std Err\"]].sum()[0])\n",
    "    \n",
    "    segment_dict = {\"claims_percentage\": claims_percentage, \"mack_mse\": mack_mse}\n",
    "        \n",
    "    mack_df = pd.DataFrame(segment_dict)\n",
    "\n",
    "    # Calculating the largest drop in mack_mse\n",
    "    mack_df['mack_mse_diff'] = mack_df['mack_mse'].diff().abs()\n",
    "    max_drop_index = mack_df['mack_mse_diff'][1:].idxmax()\n",
    "    # percentile_with_max_drop = mack_df.loc[max_drop_index, 'claims_percentage']\n",
    "    largest_drop = mack_df.loc[max_drop_index, 'mack_mse_diff']\n",
    "    threshold_percentage[f\"Rwanda {segment}\"] = mack_df.loc[max_drop_index, 'claims_percentage']\n",
    "    print(attritional_claims)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(mack_df['claims_percentage'], mack_df['mack_mse'], label='Data Points')\n",
    "    plt.scatter(mack_df.loc[max_drop_index, 'claims_percentage'], mack_df.loc[max_drop_index, 'mack_mse'], color='red', label='Largest Drop')\n",
    "    plt.axvline(x=mack_df.loc[max_drop_index, 'claims_percentage'], color='red', linestyle='--', label='Drop Indicator')\n",
    "    plt.xlabel('Claims Percentage')\n",
    "    plt.ylabel('Mack SE')\n",
    "    plt.title(f\"Rwanda {segment}: Mack SE\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(os.path.join(plot_path, f\"Rwanda_{segment}_mack_se\"))\n",
    "    plt.show()    \n",
    "\n",
    "    large_claims = segment_df.sort_values(by='gross_amount', ascending=False).iloc[:one_percent_index]\n",
    "\n",
    "    print(large_claims)\n",
    "# 010/021/9/000391/2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26243eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "claims_percentage = []\n",
    "mack_mse = []\n",
    "\n",
    "start_point = 1\n",
    "end_point = 0.9\n",
    "increment = 0.0025\n",
    "interval = int(round((start_point-end_point)/increment,0))\n",
    "\n",
    "for i in range(interval+1):\n",
    "    print(cleaned_df)\n",
    "    filtered_df = cleaned_df.sort_values(by='amount', ascending=False)\n",
    "    one_percent_index = int(len(filtered_df) * increment*i)\n",
    "    filtered_df = filtered_df.iloc[one_percent_index:]\n",
    "    \n",
    "    triangle = cl.Triangle(\n",
    "        data=filtered_df,\n",
    "        origin=\"accident_date\",\n",
    "        development=\"transaction_date\",\n",
    "        columns=[\"amount\"],\n",
    "        index=\"lob\",\n",
    "        cumulative=False\n",
    "    ).grain(\"OQDQ\")\n",
    "    \n",
    "    mack = cl.MackChainladder()\n",
    "    mack.full_triangle_\n",
    "    dev = cl.Development(average='volume')\n",
    "    mack.fit(dev.fit_transform(triangle))\n",
    "\n",
    "    plot_data = mack.summary_.to_frame(origin_as_datetime=False)\n",
    "    \n",
    "    claims_percentage.append(start_point-increment*i)\n",
    "    mack_mse.append(plot_data[[\"Mack Std Err\"]].sum()[0])\n",
    "    \n",
    "iterated_mack = {\"claims_percentage\": claims_percentage, \"mack_mse\": mack_mse}\n",
    "iterated_mack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7b4bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "ordered_df = combined_df.sort_values(by='gross_amount', ascending=False)\n",
    "one_percent_index = int(len(ordered_df) * 0.005)\n",
    "ordered_df = ordered_df.iloc[one_percent_index:]\n",
    "ordered_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba79d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_df = sample_df.drop(columns=['FIN_MONTH', \"large_attritional\"])\n",
    "cleaned_df = cleaned_df.rename(columns={\"Class\": \"lob\", \"Loss Date\": \"accident_date\", \"Pay Date\": \"transaction_date\", cleaned_df.columns[3]: \"amount\"})\n",
    "cleaned_df[\"lob\"] = cleaned_df[\"lob\"].str.title()\n",
    "\n",
    "cleaned_df['accident_date'] = pd.to_datetime(cleaned_df['accident_date'], errors='coerce', infer_datetime_format=False).dt.strftime('%d/%m/%Y')\n",
    "cleaned_df['accident_date'] = pd.to_datetime(cleaned_df['accident_date'], format='%d/%m/%Y', errors='coerce')\n",
    "cleaned_df['transaction_date'] = pd.to_datetime(cleaned_df['transaction_date'], errors='coerce').dt.strftime('%d/%m/%Y')\n",
    "cleaned_df['transaction_date'] = pd.to_datetime(cleaned_df['transaction_date'], format='%d/%m/%Y', errors='coerce')\n",
    "# cleaned_df = cleaned_df.loc[cleaned_df['accident_date'] >= pd.to_datetime('01/01/2006')]\n",
    "# cleaned_df = cleaned_df.loc[cleaned_df['transaction_date'] >= pd.to_datetime('01/01/2006')]\n",
    "\n",
    "cleaned_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4d9e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "claims_percentage = []\n",
    "mack_mse = []\n",
    "\n",
    "start_point = 1\n",
    "end_point = 0.9\n",
    "increment = 0.0025\n",
    "interval = int(round((start_point-end_point)/increment,0))\n",
    "\n",
    "for i in range(interval+1):\n",
    "    print(cleaned_df)\n",
    "    filtered_df = cleaned_df.sort_values(by='amount', ascending=False)\n",
    "    one_percent_index = int(len(filtered_df) * increment*i)\n",
    "    filtered_df = filtered_df.iloc[one_percent_index:]\n",
    "    \n",
    "    triangle = cl.Triangle(\n",
    "        data=filtered_df,\n",
    "        origin=\"accident_date\",\n",
    "        development=\"transaction_date\",\n",
    "        columns=[\"amount\"],\n",
    "        index=\"lob\",\n",
    "        cumulative=False\n",
    "    ).grain(\"OQDQ\")\n",
    "    \n",
    "    mack = cl.MackChainladder()\n",
    "    mack.full_triangle_\n",
    "    dev = cl.Development(average='volume')\n",
    "    mack.fit(dev.fit_transform(triangle))\n",
    "\n",
    "    plot_data = mack.summary_.to_frame(origin_as_datetime=False)\n",
    "    \n",
    "    claims_percentage.append(start_point-increment*i)\n",
    "    mack_mse.append(plot_data[[\"Mack Std Err\"]].sum()[0])\n",
    "    \n",
    "iterated_mack = {\"claims_percentage\": claims_percentage, \"mack_mse\": mack_mse}\n",
    "iterated_mack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39853a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(iterated_mack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137daf46",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df = sample_df.sort_values(by='amount', ascending=False)\n",
    "one_percent_index = int(len(sample_df) * 0.005)\n",
    "sample_df = sample_df.iloc[one_percent_index:]\n",
    "sample_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87175776",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "triangle = cl.Triangle(\n",
    "        data=sample_df,\n",
    "        origin=\"accident_date\",\n",
    "        development=\"transaction_date\",\n",
    "        columns=[\"amount\"],\n",
    "        index=\"lob\",\n",
    "        cumulative=False\n",
    "    ).grain(\"OQDQ\")\n",
    "\n",
    "triangle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8587f7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "mack = cl.MackChainladder()\n",
    "dev = cl.Development(average='volume')\n",
    "mack.fit(dev.fit_transform(triangle))\n",
    "\n",
    "plot_data = mack.summary_.to_frame(origin_as_datetime=False)\n",
    "plot_data[[\"Mack Std Err\"]].sum()[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

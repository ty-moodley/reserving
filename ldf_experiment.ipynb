{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. LDF selection experiment\n",
    "Messing around with a median-weighted ldf selection methodology. Unfortunately, results do not seem to significantly outperform the volume-weighted average selection that is the default of the chainladder package, but something worth exploring further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chainladder as cl\n",
    "import pandas as pd\n",
    "import os\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Testing initial methodology on sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load sample dataset from chainladder\n",
    "data = cl.load_sample(\"raa\")\n",
    "backtest = -1\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.link_ratio.heatmap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = data[data.valuation < str(data.origin[-1])]\n",
    "training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data.link_ratio.heatmap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual = data.latest_diagonal[data.origin < str(data.origin.year[backtest])]\n",
    "actual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.1 Testing BCL and developments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ave_simple = cl.Chainladder().fit(cl.Development(average='simple').fit_transform(training_data)).full_triangle_.dev_to_val()\n",
    "ave_simple = ave_simple[ave_simple.valuation==data.valuation_date].rename('columns', 'Expected')\n",
    "ave_simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bcl = cl.Chainladder().fit(cl.Development(average='volume').fit_transform(training_data))\n",
    "ave_volume = bcl.full_triangle_.dev_to_val()\n",
    "ave_volume = ave_volume[ave_volume.valuation==data.valuation_date].rename('columns', 'Expected')\n",
    "ave_volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bcl = cl.Chainladder().fit(cl.Development(average='regression').fit_transform(training_data))\n",
    "ave_regression = bcl.full_triangle_.dev_to_val()\n",
    "ave_regression = ave_regression[ave_regression.valuation==data.valuation_date].rename('columns', 'Expected')\n",
    "ave_regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.2 Testing new approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.development.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "basic_selected_ldf = {}\n",
    "dev_index = data.development.to_list()\n",
    "\n",
    "print(dev_index[-2])\n",
    "\n",
    "for i, j in enumerate(dev_index[:-2]):\n",
    "   # print(i, j)\n",
    "   dev_period = data[(data.development >= dev_index[i]) & (data.development <= dev_index[i+1])].link_ratio.to_frame()\n",
    "   claims = data[data.development == dev_index[i]].to_frame()\n",
    "\n",
    "   dev_period.index = claims.index\n",
    "   valid_data = dev_period.dropna()\n",
    "   valid_claims = claims.loc[valid_data.index]\n",
    "\n",
    "   # print(dev_period.tail(5))\n",
    "\n",
    "   volume_mean = np.average(\n",
    "    valid_data.iloc[:, 0],  # LDF values\n",
    "    weights=valid_claims.iloc[:, 0]  # Claim values as weights\n",
    "   )\n",
    "\n",
    "   mean = dev_period.mean().item()\n",
    "   median = dev_period.median().item()\n",
    "\n",
    "   print(j, mean, median, volume_mean)\n",
    "\n",
    "   volume_scale_factor = abs(mean-median)/mean\n",
    "\n",
    "   basic_selected_ldf[j] = median*(1-volume_scale_factor) + volume_mean*(volume_scale_factor)\n",
    "\n",
    "basic_selected_ldf[dev_index[-2]] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_ldf = {}\n",
    "dev_index = data.development.to_list()\n",
    "\n",
    "for i, j in enumerate(dev_index[:-1]):\n",
    "   # print(i, j)\n",
    "   dev_period = data[(data.development >= dev_index[i]) & (data.development <= dev_index[i+1])].link_ratio.to_frame().tail(4+i)\n",
    "   claims = data[data.development == dev_index[i]].to_frame().tail(4+i)\n",
    "\n",
    "   dev_period.index = claims.index\n",
    "   valid_data = dev_period.dropna()\n",
    "   valid_claims = claims.loc[valid_data.index]\n",
    "\n",
    "   # print(dev_period.tail(5))\n",
    "\n",
    "   volume_mean = np.average(\n",
    "    valid_data.iloc[:, 0],  # LDF values\n",
    "    weights=valid_claims.iloc[:, 0]  # Claim values as weights\n",
    "   )\n",
    "\n",
    "   mean = dev_period.mean().item()\n",
    "   median = dev_period.median().item()\n",
    "   std_dev = dev_period.std().item()\n",
    "\n",
    "   volume_cv = std_dev/volume_mean\n",
    "   cv = std_dev/mean\n",
    "\n",
    "   volume_scale_factor = 1/(1+volume_cv)\n",
    "   scale_factor = 1/(1+cv)\n",
    "\n",
    "   selected_ldf[j] = median*(1-volume_scale_factor) + volume_mean*(volume_scale_factor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl.DevelopmentConstant(patterns=basic_selected_ldf, style='ldf').fit(training_data).cdf_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl.DevelopmentConstant(patterns=selected_ldf, style='ldf').fit(training_data).cdf_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bcl2 = cl.Chainladder().fit(cl.DevelopmentConstant(patterns=basic_selected_ldf, style='ldf').fit_transform(training_data))\n",
    "ave_selected2 = bcl2.full_triangle_.dev_to_val()\n",
    "ave_selected2 = ave_selected2[ave_selected2.valuation==data.valuation_date]\n",
    "ave_selected2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bcl = cl.Chainladder().fit(cl.DevelopmentConstant(patterns=selected_ldf, style='ldf').fit_transform(training_data))\n",
    "ave_selected = bcl.full_triangle_.dev_to_val()\n",
    "ave_selected = ave_selected[ave_selected.valuation==data.valuation_date]\n",
    "ave_selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"volume: {abs(actual-ave_volume).sum()}\")\n",
    "print(f\"simple: {abs(actual-ave_simple).sum()}\")\n",
    "print(f\"regression: {abs(actual-ave_regression).sum()}\")\n",
    "print(f\"selected: {abs(actual-ave_selected).sum()}\")\n",
    "print(f\"selected2: {abs(actual-ave_selected2).sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Testing approach against triangles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def median_weighted_ldf(triangle, volume=False, n_periods=0):\n",
    "    '''\n",
    "    Credibility based approach with more weight on the median LDF as coefficient of variation increases.\n",
    "    :param: triangle: pd.DataFrame -> triangle from chainladder package\n",
    "    :param: volume: bool -> True if volume weighted LDF is needed\n",
    "    :param: n_periods: int -> Number of periods to consider for the LDF calculation\n",
    "\n",
    "    Returns a dictionary of selected LDFs\n",
    "    '''\n",
    "\n",
    "    selected_ldf = {}\n",
    "    dev_index = triangle.development.to_list()\n",
    "\n",
    "    for i, j in enumerate(dev_index[:-1]):\n",
    "\n",
    "        if n_periods > 0:\n",
    "            dev_period = triangle[(triangle.development >= dev_index[i]) & (triangle.development <= dev_index[i+1])].link_ratio.to_frame().tail(n_periods+i)\n",
    "            claims = triangle[triangle.development == dev_index[i]].to_frame().tail(n_periods+i)\n",
    "        else:\n",
    "            dev_period = triangle[(triangle.development >= dev_index[i]) & (triangle.development <= dev_index[i+1])].link_ratio.to_frame()\n",
    "            claims = triangle[triangle.development == dev_index[i]].to_frame()\n",
    "\n",
    "        median = dev_period.median().item()\n",
    "        std_dev = dev_period.std().item()\n",
    "\n",
    "        if volume == True:\n",
    "\n",
    "            dev_period.index = claims.index\n",
    "            valid_data = dev_period.dropna()\n",
    "            valid_claims = claims.loc[valid_data.index]\n",
    "\n",
    "            mean = np.average(\n",
    "                valid_data.iloc[:, 0],  # LDF values\n",
    "                weights=valid_claims.iloc[:, 0]  # Claim values as weights\n",
    "            )\n",
    "            cv = std_dev/volume_mean\n",
    "\n",
    "        else:\n",
    "        \n",
    "            mean = dev_period.mean().item()\n",
    "            cv = std_dev/mean\n",
    "\n",
    "        scale_factor = 1/(1+cv)\n",
    "\n",
    "        selected_ldf[j] = median*(1-scale_factor) + volume_mean*(scale_factor)\n",
    "\n",
    "    return selected_ldf\n",
    "\n",
    "def basic_median_ldf(triangle, volume=False, n_periods=0):\n",
    "    '''\n",
    "    Credibility based approach with more weight on the median LDF as relative distance from mean to median increases.\n",
    "    :param: triangle: pd.DataFrame -> triangle from chainladder package\n",
    "    :param: volume: bool -> True if volume weighted LDF is needed\n",
    "    :param: n_periods: int -> Number of periods to consider for the LDF calculation\n",
    "\n",
    "    Returns a dictionary of selected LDFs\n",
    "    '''\n",
    "\n",
    "    selected_ldf = {}\n",
    "    dev_index = triangle.development.to_list()\n",
    "\n",
    "    for i, j in enumerate(dev_index[:-1]):\n",
    "\n",
    "        if n_periods > 0:\n",
    "            dev_period = triangle[(triangle.development >= dev_index[i]) & (triangle.development <= dev_index[i+1])].link_ratio.to_frame().tail(n_periods+i)\n",
    "            claims = triangle[triangle.development == dev_index[i]].to_frame().tail(n_periods+i)\n",
    "        else:\n",
    "            dev_period = triangle[(triangle.development >= dev_index[i]) & (triangle.development <= dev_index[i+1])].link_ratio.to_frame()\n",
    "            claims = triangle[triangle.development == dev_index[i]].to_frame()\n",
    "\n",
    "        median = dev_period.median().item()\n",
    "\n",
    "        if volume == True:\n",
    "\n",
    "            dev_period.index = claims.index\n",
    "            valid_data = dev_period.dropna()\n",
    "            valid_claims = claims.loc[valid_data.index]\n",
    "\n",
    "            mean = np.average(\n",
    "                valid_data.iloc[:, 0],  # LDF values\n",
    "                weights=valid_claims.iloc[:, 0]  # Claim values as weights\n",
    "            )\n",
    "\n",
    "        else:\n",
    "        \n",
    "            mean = dev_period.mean().item()\n",
    "\n",
    "        scale_factor = abs(mean-median)/mean\n",
    "\n",
    "        selected_ldf[j] = median*(scale_factor) + volume_mean*(1-scale_factor)\n",
    "\n",
    "    return selected_ldf\n",
    "\n",
    "def quarter_to_date(quarter_str):\n",
    "    year, qtr = quarter_str.split()\n",
    "    qtr = int(qtr[-1])\n",
    "    month = qtr * 3  # Last month of the quarter\n",
    "    return f\"{year}/{month:02d}/\" + (\"31\" if month in [3, 12] else \"30\")\n",
    "\n",
    "def dev_to_date(date_str, months_str, max_date, m_flag=True):\n",
    "    if m_flag:\n",
    "        months = int(months_str[:-1])-3  # Remove the 'm' and convert to int\n",
    "    else:\n",
    "        months = int(months_str)-3\n",
    "    original_date = pd.to_datetime(date_str)\n",
    "    accident_day = original_date.day\n",
    "    accident_month = original_date.month\n",
    "    if ((accident_month+months)%12 in [0, 3]) & (accident_day == 30):\n",
    "        new_date = original_date + relativedelta(months=months, days=1)\n",
    "    else:\n",
    "        new_date = original_date + relativedelta(months=months)\n",
    "    return min(new_date, max_date).strftime('%Y/%m/%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "claims_path = r\"\"\n",
    "backtest = -1\n",
    "\n",
    "triangle_name = []\n",
    "AvE_simple = []\n",
    "AvE_regression = []\n",
    "AvE_volume = []\n",
    "AvE_median = []\n",
    "AvE_med_vol = []\n",
    "AvE_basic_median = []\n",
    "AvE_basic_med_vol = []\n",
    "\n",
    "for file in os.listdir(claims_path):\n",
    "    claims_data = pd.read_csv(os.path.join(claims_path, file))\n",
    "    \n",
    "    flat_claims_data = pd.melt(\n",
    "        claims_data, \n",
    "        id_vars=['Accident Quarter'],\n",
    "        value_vars=list(claims_data.columns)[1:],\n",
    "        var_name='Development',\n",
    "        value_name='Paid'\n",
    "    ).dropna(subset=['Accident Quarter']).sort_values(by=[\"Accident Quarter\"])\n",
    "\n",
    "    flat_claims_data[\"Accident Quarter\"] = flat_claims_data[\"Accident Quarter\"].apply(quarter_to_date)\n",
    "    max_date = pd.to_datetime(flat_claims_data['Accident Quarter']).max() # prevents development dates for future periods\n",
    "\n",
    "    flat_claims_data[\"Development\"] = flat_claims_data.apply(\n",
    "        lambda row: dev_to_date(\n",
    "            row[\"Accident Quarter\"], \n",
    "            row[\"Development\"],\n",
    "            max_date\n",
    "        ),\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    flat_claims_data[\"Paid\"] = pd.to_numeric(flat_claims_data[\"Paid\"], errors=\"coerce\")\n",
    "\n",
    "    triangle = cl.Triangle(\n",
    "        data=flat_claims_data,\n",
    "        origin=\"Accident Quarter\",\n",
    "        development=\"Development\",\n",
    "        columns=\"Paid\",\n",
    "        cumulative=True\n",
    "    )\n",
    "\n",
    "    training_data = triangle[triangle.valuation < str(triangle.origin[backtest])]\n",
    "\n",
    "    actual = triangle[(triangle.origin < str(triangle.origin[backtest]))]\n",
    "    actual = actual[(actual.valuation > str(triangle.origin[backtest]))].sum(\"development\")\n",
    "\n",
    "    ave_simple = cl.Chainladder().fit(cl.Development(average='simple').fit_transform(training_data)).full_triangle_.dev_to_val()\n",
    "    ave_simple = ave_simple[ave_simple.valuation==triangle.valuation_date]\n",
    "\n",
    "    ave_reg = cl.Chainladder().fit(cl.Development(average='regression').fit_transform(training_data)).full_triangle_.dev_to_val()\n",
    "    ave_reg = ave_reg[ave_reg.valuation==triangle.valuation_date]\n",
    "    \n",
    "    ave_volume = cl.Chainladder().fit(cl.Development(average='volume').fit_transform(training_data)).full_triangle_.dev_to_val()\n",
    "    ave_volume = ave_volume[ave_volume.valuation==triangle.valuation_date]\n",
    "\n",
    "    ave_med = cl.Chainladder().fit(cl.DevelopmentConstant(patterns=median_weighted_ldf(training_data), style='ldf').fit_transform(training_data)).full_triangle_.dev_to_val()\n",
    "    ave_med = ave_med[ave_med.valuation==triangle.valuation_date]\n",
    "    \n",
    "    ave_basic_med = cl.Chainladder().fit(cl.DevelopmentConstant(patterns=basic_median_ldf(training_data), style='ldf').fit_transform(training_data)).full_triangle_.dev_to_val()\n",
    "    ave_basic_med = ave_basic_med[ave_basic_med.valuation==triangle.valuation_date]\n",
    "\n",
    "    triangle_name.append(file.replace(\".csv\", \"\"))\n",
    "    AvE_simple.append(abs(actual-ave_simple).sum())\n",
    "    AvE_regression.append(abs(actual-ave_reg).sum())\n",
    "    AvE_volume.append(abs(actual-ave_volume).sum())\n",
    "    AvE_median.append(abs(actual-ave_med).sum())\n",
    "    AvE_basic_median.append(abs(actual-ave_basic_med).sum())\n",
    "    \n",
    "ave_df = pd.DataFrame({\n",
    "    \"triangle_name\": triangle_name,\n",
    "    \"AvE simple\": AvE_simple,\n",
    "    \"AvE regression\": AvE_regression,\n",
    "    \"AvE volume\": AvE_volume,\n",
    "    \"AvE median\": AvE_median,\n",
    "    \"AvE basic median\": AvE_basic_median,\n",
    "})\n",
    "\n",
    "ave_df.to_excel(\"ave_results_omar_1q.xlsx\", index=False)\n",
    "ave_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "claims_path = r\"\"\n",
    "backtest = -1\n",
    "\n",
    "triangle_name = []\n",
    "AvE_simple = []\n",
    "AvE_regression = []\n",
    "AvE_volume = []\n",
    "AvE_median = []\n",
    "AvE_med_vol = []\n",
    "AvE_basic_median = []\n",
    "AvE_basic_med_vol = []\n",
    "\n",
    "for file in os.listdir(claims_path):\n",
    "    claims_data = pd.read_excel(os.path.join(claims_path, file))\n",
    "    \n",
    "    flat_claims_data = pd.melt(\n",
    "        claims_data, \n",
    "        id_vars=['Accident Quarter'],\n",
    "        value_vars=list(claims_data.columns)[1:],\n",
    "        var_name='Development',\n",
    "        value_name='Paid'\n",
    "    ).dropna(subset=['Accident Quarter']).sort_values(by=[\"Accident Quarter\"])\n",
    "\n",
    "    flat_claims_data[\"Accident Quarter\"] = flat_claims_data[\"Accident Quarter\"].apply(quarter_to_date)\n",
    "    max_date = pd.to_datetime(flat_claims_data['Accident Quarter']).max() # prevents development dates for future periods\n",
    "\n",
    "    flat_claims_data[\"Development\"] = flat_claims_data.apply(\n",
    "        lambda row: dev_to_date(\n",
    "            row[\"Accident Quarter\"], \n",
    "            row[\"Development\"],\n",
    "            max_date\n",
    "        ),\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    flat_claims_data[\"Paid\"] = pd.to_numeric(flat_claims_data[\"Paid\"], errors=\"coerce\")\n",
    "\n",
    "    triangle = cl.Triangle(\n",
    "        data=flat_claims_data,\n",
    "        origin=\"Accident Quarter\",\n",
    "        development=\"Development\",\n",
    "        columns=\"Paid\",\n",
    "        cumulative=True\n",
    "    )\n",
    "\n",
    "    training_data = triangle[triangle.valuation < str(triangle.origin[backtest])]\n",
    "\n",
    "    actual = triangle[(triangle.origin < str(triangle.origin[backtest]))]\n",
    "    actual = actual[(actual.valuation > str(triangle.origin[backtest]))].sum(\"development\")\n",
    "\n",
    "    ave_simple = cl.Chainladder().fit(cl.Development(average='simple', n_periods=8).fit_transform(training_data)).full_triangle_.dev_to_val()\n",
    "    ave_simple = ave_simple[ave_simple.valuation==triangle.valuation_date]\n",
    "\n",
    "    ave_reg = cl.Chainladder().fit(cl.Development(average='regression', n_periods=8).fit_transform(training_data)).full_triangle_.dev_to_val()\n",
    "    ave_reg = ave_reg[ave_reg.valuation==triangle.valuation_date]\n",
    "    \n",
    "    ave_volume = cl.Chainladder().fit(cl.Development(average='volume', n_periods=8).fit_transform(training_data)).full_triangle_.dev_to_val()\n",
    "    ave_volume = ave_volume[ave_volume.valuation==triangle.valuation_date]\n",
    "\n",
    "    ave_med = cl.Chainladder().fit(cl.DevelopmentConstant(patterns=median_weighted_ldf(training_data, n_periods=8), style='ldf').fit_transform(training_data)).full_triangle_.dev_to_val()\n",
    "    ave_med = ave_med[ave_med.valuation==triangle.valuation_date]\n",
    "    \n",
    "    ave_med_vol = cl.Chainladder().fit(cl.DevelopmentConstant(patterns=median_weighted_ldf(training_data, volume=True, n_periods=8), style='ldf').fit_transform(training_data)).full_triangle_.dev_to_val()\n",
    "    ave_med_vol = ave_med_vol[ave_med_vol.valuation==triangle.valuation_date]\n",
    "\n",
    "    ave_basic_med = cl.Chainladder().fit(cl.DevelopmentConstant(patterns=basic_median_ldf(training_data, n_periods=8), style='ldf').fit_transform(training_data)).full_triangle_.dev_to_val()\n",
    "    ave_basic_med = ave_basic_med[ave_basic_med.valuation==triangle.valuation_date]\n",
    "    \n",
    "    ave_basic_med_vol = cl.Chainladder().fit(cl.DevelopmentConstant(patterns=basic_median_ldf(training_data, volume=True, n_periods=8), style='ldf').fit_transform(training_data)).full_triangle_.dev_to_val()\n",
    "    ave_basic_med_vol = ave_basic_med_vol[ave_basic_med_vol.valuation==triangle.valuation_date]\n",
    "\n",
    "    triangle_name.append(file.replace(\".xlsx\", \"\"))\n",
    "    AvE_simple.append(abs(actual-ave_simple).sum())\n",
    "    AvE_regression.append(abs(actual-ave_reg).sum())\n",
    "    AvE_volume.append(abs(actual-ave_volume).sum())\n",
    "    AvE_median.append(abs(actual-ave_med).sum())\n",
    "    AvE_med_vol.append(abs(actual-ave_med_vol).sum())\n",
    "    AvE_basic_median.append(abs(actual-ave_basic_med).sum())\n",
    "    AvE_basic_med_vol.append(abs(actual-ave_basic_med_vol).sum())\n",
    "    \n",
    "ave_df = pd.DataFrame({\n",
    "    \"triangle_name\": triangle_name,\n",
    "    \"AvE simple\": AvE_simple,\n",
    "    \"AvE regression\": AvE_regression,\n",
    "    \"AvE volume\": AvE_volume,\n",
    "    \"AvE median\": AvE_median,\n",
    "    \"AvE median volume\": AvE_med_vol,\n",
    "    \"AvE basic median\": AvE_basic_median,\n",
    "    \"AvE basic median volume\": AvE_basic_med_vol\n",
    "})\n",
    "\n",
    "ave_df.to_excel(\"ave_results_omi_n8_q1.xlsx\", index=False)\n",
    "ave_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "claims_path = r\"K:\\Shaazia\\OMI Reserve Investigations\\2023-09\\ResQ Macros\\Claims Data\"\n",
    "backtest = -1\n",
    "\n",
    "claims_data = pd.read_excel(os.path.join(claims_path, os.listdir(claims_path)[0]))\n",
    "\n",
    "flat_claims_data = pd.melt(\n",
    "    claims_data, \n",
    "    id_vars=['Accident Quarter'],\n",
    "    value_vars=list(claims_data.columns)[1:],\n",
    "    var_name='Development',\n",
    "    value_name='Paid'\n",
    ").dropna(subset=['Accident Quarter']).sort_values(by=[\"Accident Quarter\"])\n",
    "\n",
    "flat_claims_data[\"Accident Quarter\"] = flat_claims_data[\"Accident Quarter\"].apply(quarter_to_date)\n",
    "max_date = pd.to_datetime(flat_claims_data['Accident Quarter']).max() # prevents development dates for future periods\n",
    "\n",
    "flat_claims_data[\"Development\"] = flat_claims_data.apply(\n",
    "    lambda row: dev_to_date(\n",
    "        row[\"Accident Quarter\"], \n",
    "        row[\"Development\"],\n",
    "        max_date\n",
    "    ),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "flat_claims_data[\"Paid\"] = pd.to_numeric(flat_claims_data[\"Paid\"], errors=\"coerce\")\n",
    "\n",
    "triangle = cl.Triangle(\n",
    "    data=flat_claims_data,\n",
    "    origin=\"Accident Quarter\",\n",
    "    development=\"Development\",\n",
    "    columns=\"Paid\",\n",
    "    cumulative=True\n",
    ")\n",
    "triangle.link_ratio.heatmap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = triangle[triangle.valuation < str(triangle.origin[-1])]\n",
    "training_data.link_ratio.heatmap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 99. Storing other pieces of code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## R-code Correlation\n",
    "\n",
    "# library(data.table)\n",
    "# library(dplyr)\n",
    "# library(ggplot2)\n",
    "# library(stringr)\n",
    "# library(lubridate)\n",
    "# library(corrplot)\n",
    "\n",
    "# # valid_dates = rbind(expand.grid(V1 = 2014:2023, V2 = paste0(\"Q\",1:4)), \n",
    "# #                    expand.grid(V1 = 2024, V2 = paste0(\"Q\", 1:2))) %>% data.table()\n",
    "\n",
    "# valid_dates = expand.grid(V1 = 2019:2023, V2 = paste0(\"Q\",1:4)) %>% data.table()\n",
    "\n",
    "# valid_dates[, accident_period := paste0(V1,\" \", V2)]\n",
    "# valid_dates %>% setkey(accident_period)\n",
    "# valid_dates = valid_dates[-1]\n",
    "# #valid_dates = valid_dates[-1]\n",
    "\n",
    "# dat = fread(\"\")\n",
    "\n",
    "# dat_sub = dat[country != \"Arsenal\" & basis == \"gross\" & accident_period %in% valid_dates$accident_period]\n",
    "\n",
    "# pivot = dat_sub %>% dcast.data.table(accident_period ~ country, value.var = \"ulr_residual\")\n",
    "# corrs = pivot[, c(-1)] %>% as.matrix()  %>% cor\n",
    "\n",
    "# corrs %>% corrplot::corrplot()\n",
    "\n",
    "# tests = corrs %>% corrplot::cor.mtest()\n",
    "# pvals = data.table(names = tests$p %>% rownames(), tests$p) %>% melt.data.table()\n",
    "\n",
    "\n",
    "# corrs = data.table(names = corrs %>% rownames(), corrs) %>% melt.data.table()\n",
    "# corrs[, pair := paste0(names, variable)]\n",
    "# pvals[, pair := paste0(names, variable)]\n",
    "\n",
    "# corrs[pair %in% pvals[value >0 & value < 0.05][order(names)]$pair]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## R-code ISP\n",
    "\n",
    "# library(data.table)\n",
    "# library(dplyr)\n",
    "# library(ggplot2)\n",
    "# library(ggpubr)\n",
    "# library(keras)\n",
    "# library(tensorflow)\n",
    "# library(stringr)\n",
    "# library(lubridate)\n",
    "# require(janitor)\n",
    "# library(ChainLadder)\n",
    "\n",
    "\n",
    "# dat = fread(\"\", header = T) %>% melt.data.table(id.vars = \"V1\") %>% clean_names()\n",
    "# dat[, AY := v1]\n",
    "# dat[, v1 := NULL]\n",
    "# dat[, DY := variable]\n",
    "# dat[, variable := NULL]\n",
    "# dat[, value := as.numeric(value)]\n",
    "\n",
    "# triang = dat %>% as.triangle(., origin = \"AY\", dev = \"DY\", value = \"value\")\n",
    "# triang %>% MackChainLadder()\n",
    "\n",
    "# exp(qnorm(0.8)*0.2)*50000000-50000000\n",
    "\n",
    "# ### CDR\n",
    "\n",
    "# triang %>% MackChainLadder() %>% CDR()\n",
    "# cdrs = triang %>% MackChainLadder() %>% CDR(dev = \"all\")\n",
    "\n",
    "# ibnr = cdrs$IBNR[21]\n",
    "\n",
    "# cdrs = (cdrs$`CDR(1)S.E.`[21]^2+\n",
    "# cdrs$`CDR(2)S.E.`[21]^2+\n",
    "# cdrs$`CDR(3)S.E.`[21]^2+\n",
    "# cdrs$`CDR(4)S.E.`[21]^2)^0.5\n",
    "\n",
    "# CoV = cdrs/ibnr\n",
    "\n",
    "# ADC = exp(qnorm(0.65)*CoV)*ibnr-ibnr\n",
    "\n",
    "# ## lognormal\n",
    "\n",
    "# mean = ibnr\n",
    "# sd = cdrs\n",
    "# empirical_cov = sd/mean\n",
    "\n",
    "# sigma = (log(empirical_cov^2+1))^0.5\n",
    "# mu = log(mean)-sigma^2/2\n",
    "\n",
    "# qlnorm(0.8, meanlog = mu, sdlog = sigma) - mean"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
